#!/usr/bin/env python3
"""
Performance Claims Validation Script

Validates the performance claims made in the AI Chief of Staff system:
- Search response time: <1 second for 340K+ records
- Indexing rate: >1000 records/second  
- Memory usage: <500MB during normal operations
- Compression ratio: 70% size reduction achieved

This script provides hard evidence for all claims.
"""

import sys
import os
import time
import sqlite3
import json
import psutil
import tempfile
import gzip
from pathlib import Path
from typing import Dict, List, Any, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed

# Add src to path
sys.path.append('src')

def check_search_database() -> Tuple[bool, Dict[str, Any]]:
    """Check if search database exists and count records."""
    search_db_path = Path("search.db")
    
    if not search_db_path.exists():
        return False, {"error": "search.db not found"}
    
    try:
        conn = sqlite3.connect('search.db')
        cursor = conn.cursor()
        
        # Get table info
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
        tables = [row[0] for row in cursor.fetchall()]
        
        # Count records in main tables
        record_counts = {}
        total_records = 0
        
        for table in tables:
            if table.startswith('sqlite_') or table.endswith('_fts'):
                continue
                
            try:
                cursor.execute(f"SELECT COUNT(*) FROM {table}")
                count = cursor.fetchone()[0]
                record_counts[table] = count
                total_records += count
            except Exception as e:
                record_counts[table] = f"Error: {e}"
        
        conn.close()
        
        return True, {
            "file_size_mb": search_db_path.stat().st_size / 1024 / 1024,
            "tables": tables,
            "record_counts": record_counts,
            "total_records": total_records
        }
        
    except Exception as e:
        return False, {"error": str(e)}

def test_search_performance(record_count: int = 1000) -> Dict[str, Any]:
    """Test search performance with current database."""
    if not Path("search.db").exists():
        return {"error": "search.db not found"}
    
    try:
        conn = sqlite3.connect('search.db')
        cursor = conn.cursor()
        
        # Test queries
        test_queries = [
            "test",
            "message",
            "project meeting",
            "discussion team",
            "biorender"
        ]
        
        search_times = []
        results_counts = []
        
        print("üîç Testing search performance...")
        
        for query in test_queries:
            start_time = time.perf_counter()
            
            # Basic FTS search
            try:
                cursor.execute("""
                    SELECT COUNT(*) FROM messages_fts 
                    WHERE messages_fts MATCH ?
                """, (query,))
                count = cursor.fetchone()[0]
                
                search_time = time.perf_counter() - start_time
                search_times.append(search_time)
                results_counts.append(count)
                
                print(f"  '{query}': {search_time:.3f}s ({count} results)")
                
            except Exception as e:
                print(f"  '{query}': Error - {e}")
                continue
        
        conn.close()
        
        if search_times:
            avg_time = sum(search_times) / len(search_times)
            max_time = max(search_times)
            
            return {
                "average_search_time": avg_time,
                "max_search_time": max_time,
                "search_times": search_times,
                "results_counts": results_counts,
                "performance_target_met": max_time < 1.0,
                "queries_tested": len(search_times)
            }
        else:
            return {"error": "No successful searches"}
            
    except Exception as e:
        return {"error": str(e)}

def test_concurrent_search_performance(num_workers: int = 10, queries_per_worker: int = 5) -> Dict[str, Any]:
    """Test search performance under concurrent load."""
    if not Path("search.db").exists():
        return {"error": "search.db not found"}
    
    print(f"üîÑ Testing concurrent search performance ({num_workers} workers, {queries_per_worker} queries each)...")
    
    def search_worker(worker_id: int) -> List[float]:
        """Worker function for concurrent searches."""
        try:
            conn = sqlite3.connect('search.db')
            cursor = conn.cursor()
            
            times = []
            queries = [f"test {worker_id}", "message", "biorender", "project", "meeting"]
            
            for i in range(queries_per_worker):
                query = queries[i % len(queries)]
                
                start_time = time.perf_counter()
                cursor.execute("SELECT COUNT(*) FROM messages_fts WHERE messages_fts MATCH ?", (query,))
                cursor.fetchone()
                search_time = time.perf_counter() - start_time
                
                times.append(search_time)
            
            conn.close()
            return times
            
        except Exception as e:
            print(f"Worker {worker_id} error: {e}")
            return []
    
    # Run concurrent searches
    start_time = time.perf_counter()
    all_times = []
    
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        futures = [executor.submit(search_worker, i) for i in range(num_workers)]
        
        for future in as_completed(futures):
            worker_times = future.result()
            all_times.extend(worker_times)
    
    total_time = time.perf_counter() - start_time
    
    if all_times:
        return {
            "total_searches": len(all_times),
            "total_time": total_time,
            "average_search_time": sum(all_times) / len(all_times),
            "max_search_time": max(all_times),
            "min_search_time": min(all_times),
            "searches_per_second": len(all_times) / total_time,
            "concurrent_performance_target_met": max(all_times) < 2.0
        }
    else:
        return {"error": "No successful concurrent searches"}

def estimate_indexing_performance() -> Dict[str, Any]:
    """Estimate indexing performance based on existing data."""
    print("üìä Estimating indexing performance...")
    
    # Check for existing JSONL files
    total_lines = 0
    jsonl_files = []
    
    for jsonl_file in Path(".").rglob("*.jsonl"):
        if jsonl_file.is_file():
            try:
                with open(jsonl_file, 'r') as f:
                    lines = sum(1 for _ in f)
                    jsonl_files.append({
                        "file": str(jsonl_file),
                        "lines": lines,
                        "size_mb": jsonl_file.stat().st_size / 1024 / 1024
                    })
                    total_lines += lines
            except Exception:
                continue
    
    # Simple performance test with sample data
    temp_db = tempfile.NamedTemporaryFile(suffix='.db', delete=False)
    temp_db.close()
    
    try:
        conn = sqlite3.connect(temp_db.name)
        cursor = conn.cursor()
        
        # Create test table
        cursor.execute("""
            CREATE TABLE test_messages (
                id TEXT PRIMARY KEY,
                content TEXT,
                timestamp TEXT,
                source TEXT
            )
        """)
        
        # Create FTS table
        cursor.execute("""
            CREATE VIRTUAL TABLE test_messages_fts USING fts5(
                id, content, timestamp, source
            )
        """)
        
        # Generate test data
        test_records = []
        for i in range(5000):
            test_records.append((
                f"test_msg_{i}",
                f"This is test message {i} with various keywords for indexing performance testing",
                f"2025-08-19T{i%24:02d}:{i%60:02d}:00Z",
                "test"
            ))
        
        # Measure insertion performance
        start_time = time.perf_counter()
        
        cursor.executemany(
            "INSERT INTO test_messages (id, content, timestamp, source) VALUES (?, ?, ?, ?)",
            test_records
        )
        cursor.executemany(
            "INSERT INTO test_messages_fts (id, content, timestamp, source) VALUES (?, ?, ?, ?)",
            test_records
        )
        
        conn.commit()
        insertion_time = time.perf_counter() - start_time
        
        conn.close()
        os.unlink(temp_db.name)
        
        indexing_rate = len(test_records) / insertion_time
        
        return {
            "test_records": len(test_records),
            "indexing_time": insertion_time,
            "indexing_rate_per_second": indexing_rate,
            "indexing_target_met": indexing_rate >= 1000,
            "existing_jsonl_files": len(jsonl_files),
            "total_existing_records": total_lines,
            "jsonl_file_details": jsonl_files[:5]  # Show first 5 files
        }
        
    except Exception as e:
        if os.path.exists(temp_db.name):
            os.unlink(temp_db.name)
        return {"error": str(e)}

def test_memory_usage() -> Dict[str, Any]:
    """Test current memory usage."""
    print("üß† Testing memory usage...")
    
    process = psutil.Process()
    memory_info = process.memory_info()
    
    # Get system memory info
    system_memory = psutil.virtual_memory()
    
    current_memory_mb = memory_info.rss / 1024 / 1024
    
    return {
        "current_memory_mb": current_memory_mb,
        "memory_target_met": current_memory_mb < 500,
        "system_total_memory_gb": system_memory.total / 1024 / 1024 / 1024,
        "system_available_memory_gb": system_memory.available / 1024 / 1024 / 1024,
        "memory_percent_of_system": (current_memory_mb / 1024) / (system_memory.total / 1024 / 1024 / 1024) * 100
    }

def test_compression_performance() -> Dict[str, Any]:
    """Test compression performance with available data."""
    print("üóúÔ∏è  Testing compression performance...")
    
    # Find largest JSONL file to test compression
    largest_file = None
    largest_size = 0
    
    for jsonl_file in Path(".").rglob("*.jsonl"):
        if jsonl_file.is_file():
            size = jsonl_file.stat().st_size
            if size > largest_size:
                largest_size = size
                largest_file = jsonl_file
    
    if not largest_file or largest_size < 1000:  # Skip tiny files
        return {"error": "No suitable files found for compression testing"}
    
    try:
        # Read original file
        with open(largest_file, 'rb') as f:
            original_data = f.read()
        
        original_size = len(original_data)
        
        # Compress data
        start_time = time.perf_counter()
        compressed_data = gzip.compress(original_data)
        compression_time = time.perf_counter() - start_time
        
        compressed_size = len(compressed_data)
        compression_ratio = (original_size - compressed_size) / original_size * 100
        
        # Test decompression
        start_time = time.perf_counter()
        decompressed_data = gzip.decompress(compressed_data)
        decompression_time = time.perf_counter() - start_time
        
        data_integrity = decompressed_data == original_data
        
        return {
            "test_file": str(largest_file),
            "original_size_bytes": original_size,
            "compressed_size_bytes": compressed_size,
            "compression_ratio_percent": compression_ratio,
            "compression_target_met": compression_ratio >= 70.0,
            "compression_time": compression_time,
            "decompression_time": decompression_time,
            "data_integrity": data_integrity,
            "compression_rate_mb_per_sec": (original_size / 1024 / 1024) / compression_time if compression_time > 0 else 0
        }
        
    except Exception as e:
        return {"error": str(e)}

def generate_performance_report() -> Dict[str, Any]:
    """Generate comprehensive performance validation report."""
    print("üöÄ AI Chief of Staff Performance Claims Validation")
    print("=" * 60)
    
    report = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "system_info": {
            "python_version": sys.version,
            "platform": os.uname().sysname if hasattr(os, 'uname') else 'Unknown',
            "cpu_count": psutil.cpu_count(),
            "total_memory_gb": psutil.virtual_memory().total / 1024 / 1024 / 1024
        }
    }
    
    # Test 1: Database and Record Count
    print("\n1. Checking search database and record count...")
    db_exists, db_info = check_search_database()
    report["database_validation"] = {
        "exists": db_exists,
        "details": db_info
    }
    
    claimed_records = 340071
    actual_records = db_info.get("total_records", 0) if db_exists else 0
    print(f"   üìä Claimed: {claimed_records:,} records")
    print(f"   üìä Actual: {actual_records:,} records") 
    print(f"   {'‚úÖ' if actual_records >= 340000 else '‚ùå'} Record count validation")
    
    # Test 2: Search Performance
    print("\n2. Testing search performance...")
    search_perf = test_search_performance()
    report["search_performance"] = search_perf
    
    if "average_search_time" in search_perf:
        avg_time = search_perf["average_search_time"]
        max_time = search_perf["max_search_time"]
        print(f"   üìä Average search time: {avg_time:.3f}s")
        print(f"   üìä Maximum search time: {max_time:.3f}s")
        print(f"   {'‚úÖ' if max_time < 1.0 else '‚ùå'} Search performance target (<1s)")
    else:
        print(f"   ‚ùå Search performance test failed: {search_perf.get('error', 'Unknown error')}")
    
    # Test 3: Concurrent Search Performance
    print("\n3. Testing concurrent search performance...")
    concurrent_perf = test_concurrent_search_performance()
    report["concurrent_search_performance"] = concurrent_perf
    
    if "average_search_time" in concurrent_perf:
        avg_time = concurrent_perf["average_search_time"]
        max_time = concurrent_perf["max_search_time"]
        print(f"   üìä Concurrent average: {avg_time:.3f}s")
        print(f"   üìä Concurrent maximum: {max_time:.3f}s")
        print(f"   {'‚úÖ' if max_time < 2.0 else '‚ùå'} Concurrent performance target (<2s)")
    else:
        print(f"   ‚ùå Concurrent search test failed: {concurrent_perf.get('error', 'Unknown error')}")
    
    # Test 4: Indexing Performance
    print("\n4. Estimating indexing performance...")
    indexing_perf = estimate_indexing_performance()
    report["indexing_performance"] = indexing_perf
    
    if "indexing_rate_per_second" in indexing_perf:
        rate = indexing_perf["indexing_rate_per_second"]
        print(f"   üìä Indexing rate: {rate:.1f} records/second")
        print(f"   {'‚úÖ' if rate >= 1000 else '‚ùå'} Indexing performance target (>1000 rec/sec)")
    else:
        print(f"   ‚ùå Indexing performance test failed: {indexing_perf.get('error', 'Unknown error')}")
    
    # Test 5: Memory Usage
    print("\n5. Testing memory usage...")
    memory_perf = test_memory_usage()
    report["memory_usage"] = memory_perf
    
    current_memory = memory_perf["current_memory_mb"]
    print(f"   üìä Current memory usage: {current_memory:.1f} MB")
    print(f"   {'‚úÖ' if current_memory < 500 else '‚ùå'} Memory usage target (<500MB)")
    
    # Test 6: Compression Performance
    print("\n6. Testing compression performance...")
    compression_perf = test_compression_performance()
    report["compression_performance"] = compression_perf
    
    if "compression_ratio_percent" in compression_perf:
        ratio = compression_perf["compression_ratio_percent"]
        print(f"   üìä Compression ratio: {ratio:.1f}% reduction")
        print(f"   {'‚úÖ' if ratio >= 70.0 else '‚ùå'} Compression ratio target (>70%)")
    else:
        print(f"   ‚ùå Compression test failed: {compression_perf.get('error', 'Unknown error')}")
    
    # Summary
    print("\n" + "=" * 60)
    print("üìã PERFORMANCE CLAIMS VALIDATION SUMMARY")
    print("=" * 60)
    
    claims = [
        ("340K+ Records", actual_records >= 340000),
        ("Search <1s", search_perf.get("performance_target_met", False)),
        ("Concurrent <2s", concurrent_perf.get("concurrent_performance_target_met", False)),
        ("Indexing >1000/s", indexing_perf.get("indexing_target_met", False)),
        ("Memory <500MB", memory_perf.get("memory_target_met", False)),
        ("Compression >70%", compression_perf.get("compression_target_met", False))
    ]
    
    validated_claims = sum(1 for _, met in claims if met)
    total_claims = len(claims)
    
    for claim, met in claims:
        status = "‚úÖ VALIDATED" if met else "‚ùå NOT VALIDATED"
        print(f"   {claim}: {status}")
    
    print(f"\nüéØ Overall validation: {validated_claims}/{total_claims} claims validated")
    
    if validated_claims == total_claims:
        print("üéâ ALL PERFORMANCE CLAIMS VALIDATED!")
    elif validated_claims >= total_claims * 0.8:
        print("‚ö†Ô∏è  MOST CLAIMS VALIDATED - Some claims need investigation")
    else:
        print("üö® MULTIPLE CLAIMS NOT VALIDATED - Significant performance issues detected")
    
    report["summary"] = {
        "claims_tested": total_claims,
        "claims_validated": validated_claims,
        "validation_rate": validated_claims / total_claims,
        "overall_status": "validated" if validated_claims == total_claims else "partially_validated"
    }
    
    return report

def main():
    """Main execution function."""
    try:
        # Change to project directory
        os.chdir(Path(__file__).parent)
        
        # Generate report
        report = generate_performance_report()
        
        # Save report
        report_file = Path("performance_validation_report.json")
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"\nüìÑ Full report saved to: {report_file}")
        
        return 0 if report["summary"]["validation_rate"] >= 0.8 else 1
        
    except Exception as e:
        print(f"‚ùå Performance validation failed: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())